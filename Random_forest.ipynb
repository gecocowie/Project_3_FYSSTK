{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    name: ASO Tuolumne basin machine learning and geostatistics data set\n",
      "    contains: SWE depth maps from 16 Airborne Snow Observatory lidar scans (2016-2019)  and \"static\" terrain parameters from the SRTM DEM and the National Land Cover Database\n",
      "    location: Tuolumne basin (around Hetch Hetch Reservoir), in the Californian Sierra Nevada\n",
      "    time period: Multiple flights in the snow seasons 2016, 2017, 2018, and 2019\n",
      "    georeferencing: Pixels/cells are regularly spaced in zone 11S of the UTM projection based on the WGS84 ellipsoid.\n",
      "    convention: By convention, the first pixel (i.e.: row index i=1, column index j=1) is in the upper left corner of the image.\n",
      "    dimensions: The image stack \"D\" (SWE maps) have 3 dimensions [dim1=rows=y_i=northing, dim2=columns=x_j=easting, dim3=t_n=time of flight]\n",
      "    Ground sampling distance: GSD=100 meters (spatial resolution)\n",
      "    easting coordinate: The easting pixel center coordinates x_j, which are increasing from left to right along the columns of the image, are given by x_j=x_ULC+(1-j)*GSD where j runs from 1 to ncolumns=3500\n",
      "    northing coordinate: The northing pixel center coordinates y_i, which are decreasing from top to bottom down the rows of the image, are given by y_i=x_ULC-(1-i)*GSD where i runs from 1 to nrows=2100\n",
      "    information: Contact kristoffer.aalstad@geo.uio.no\n",
      "    created by: Kristoffer Aalstad\n",
      "    created: 03-Dec-2020 15:48:53\n",
      "    license: CC BY 4.0\n",
      "    thanks: Thanks to the ASO team (link https://www.airbornesnowobservatories.com/ ), particularly Thomas Painter and Kat Bormann, for sharing their data\n",
      "    references: ASO: https://doi.org/10.1016/j.rse.2016.06.018 , NLCD: https://doi.org/10.1016/j.isprsjprs.2018.09.006 , SRTM: https://doi.org/10.1029/2005RG000183 \n",
      "    dimensions(sizes): y(559), x(648), t(16)\n",
      "    variables(dimensions): float64 \u001b[4mz\u001b[0m(x,y), float64 \u001b[4masp\u001b[0m(x,y), float64 \u001b[4mslp\u001b[0m(x,y), float64 \u001b[4msvf\u001b[0m(x,y), float64 \u001b[4mcc\u001b[0m(x,y), float64 \u001b[4mmask\u001b[0m(x,y), float64 \u001b[4mX\u001b[0m(x,y), float64 \u001b[4mY\u001b[0m(x,y), float64 \u001b[4mD\u001b[0m(t,x,y), uint16 \u001b[4mDOY\u001b[0m(t), uint16 \u001b[4myear\u001b[0m(t), float64 \u001b[4mx\u001b[0m(x), float64 \u001b[4my\u001b[0m(y)\n",
      "    groups: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run Read_Tuolumne_data.ipynb #run this cell to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random forest regression\"\"\"\n",
    "def skl_rfr(x,x_train,x_test,y_train,depth,n_trees):\n",
    "    \"\"\"random forest regression using sklearn\"\"\"\n",
    "    regr = RandomForestRegressor(max_depth=depth, n_estimators=n_trees, random_state=0)\n",
    "    regr.fit(x_train,y_train)\n",
    "    y_pred_basin = regr.predict(x)#make prediction for the whole basin\n",
    "    y_pred_test = regr.predict(x_test)\n",
    "    y_pred_train = regr.predict(x_train)\n",
    "    return y_pred_basin,y_pred_test, y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize the test and training datasets to use for varying number of trees, and varying number of estimators\n",
    "    Only use 1 of the 16 passes in order for the regression to be finished in a reasonable amount of time\"\"\"\n",
    "X = np.transpose(np.array([svf_clean,z_clean,northness_clean,eastness_clean,slp_clean,cc_clean]))#create matrix of all predicators\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,D_clean[:,0],test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Varying the number of trees\"\"\"\n",
    "n_trees = np.arange(1,50)\n",
    "y_pred_basin_n_trees = np.zeros((D_clean.shape[0],len(n_trees)))\n",
    "y_pred_test_n_trees = np.zeros((y_test.shape[0],len(n_trees)))\n",
    "y_pred_train_n_trees = np.zeros((y_train.shape[0],len(n_trees)))\n",
    "\n",
    "for i in n_trees:\n",
    "    #iteratively increase the number of trees in the regression and store in arrays\n",
    "    print(i)\n",
    "    y_pred_basin_n_trees[:,i-1],y_pred_test_n_trees[:,i-1], y_pred_train_n_trees[:,i-1] = skl_rfr(X,x_train,x_test,y_train,15,i)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Find MSE as function depth\"\"\"\n",
    "n_depth = np.arange(1,50)\n",
    "y_pred_basin_n_d = np.zeros((D_clean.shape[0],len(n_depth)))\n",
    "y_pred_test_n_d = np.zeros((y_test.shape[0],len(n_depth)))\n",
    "y_pred_train_n_d = np.zeros((y_train.shape[0],len(n_depth)))\n",
    "\n",
    "for i in n_depth:\n",
    "    #iteratively increase the depth in the regression and store in arrays\n",
    "    print(i)\n",
    "    y_pred_basin_n_d[:,i-1],y_pred_test_n_d[:,i-1], y_pred_train_n_d[:,i-1] = skl_rfr(X,x_train,x_test,y_train,i,20)\n",
    "    #Bias_trees[i] = np.mean((y_test - np.mean(y_pred_test_n_d[:,i-1]))**2)\n",
    "    #variance_trees[i] = np.var(y_pred_test_n_trees[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save files for posterity\"\"\"\n",
    "\n",
    "\"\"\"np.savetxt('test_runs/' + 'y_pred_basin_n_trees.txt', y_pred_basin_n_trees)\n",
    "np.savetxt('test_runs/' + 'y_pred_test_n_trees.txt', y_pred_test_n_trees)\n",
    "np.savetxt('test_runs/' + 'y_pred_train_n_trees.txt', y_pred_train_n_trees)\n",
    "\n",
    "np.savetxt('test_runs/' + 'y_pred_basin_n_d.txt', y_pred_basin_n_d)\n",
    "np.savetxt('test_runs/' + 'y_pred_test_n_d.txt', y_pred_test_n_d)\n",
    "np.savetxt('test_runs/' + 'y_pred_train_n_d.txt', y_pred_train_n_d)\n",
    "np.savetxt('test_runs/' + 'y_test.txt',y_test)\n",
    "np.savetxt('test_runs/' + 'y_train.txt',y_train)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Find MSE and R2 as function of number of trees\"\"\"\n",
    "n_trees = np.arange(1,50)\n",
    "\n",
    "#Create empty arrays for storage\n",
    "MSE_basin_rnd_forest = np.zeros(len(n_trees))\n",
    "MSE_test_rnd_forest = np.zeros(len(n_trees))\n",
    "MSE_train_rnd_forest = np.zeros(len(n_trees))\n",
    "R2_test_rnd_forest = np.zeros(len(n_trees))\n",
    "Bias_trees = np.zeros(len(n_trees))\n",
    "variance_trees = np.zeros(len(n_trees))\n",
    "\n",
    "\"\"\"Unfortunately the test files i created were to large to upload to github \n",
    "   so i had to restructrure this part last minute now the entire script has\n",
    "   to be run before this part works\"\"\"\n",
    "#y_pred_basin_n_trees = np.loadtxt(\"test_runs/y_pred_basin_n_trees.txt\")\n",
    "#y_pred_test_n_trees = np.loadtxt(\"test_runs/y_pred_test_n_trees.txt\")\n",
    "#y_pred_train_n_trees = np.loadtxt(\"test_runs/y_pred_train_n_trees.txt\")\n",
    "#y_test = np.loadtxt('test_runs/y_test.txt')\n",
    "#y_train = np.loadtxt('test_runs/y_train.txt')\n",
    "\n",
    "for i in range(len(n_trees)):\n",
    "    MSE_basin_rnd_forest[i] = metric.mean_squared_error(D_clean[:,0],y_pred_basin_n_trees[:,i])\n",
    "    MSE_train_rnd_forest[i] = metric.mean_squared_error(y_train,y_pred_train_n_trees[:,i])\n",
    "    MSE_test_rnd_forest[i] = metric.mean_squared_error(y_test,y_pred_test_n_trees[:,i])\n",
    "    R2_test_rnd_forest[i] = metric.r2_score(y_test,y_pred_test_n_trees[:,i])\n",
    "    Bias_trees[i] = np.mean((y_test - np.mean(y_pred_test_n_trees[:,i]))**2)\n",
    "    variance_trees[i] = np.var(y_pred_test_n_trees[:,i])\n",
    "#polydeg_array = np.arange(1,MaxPoly+1) #Plot the MSE results against each other\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize=(10,6))\n",
    "color = 'tab:red'\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('R$^2$', color=color)\n",
    "plt.plot(n_trees,R2_test_rnd_forest, color=color)\n",
    "plt.tick_params(axis='y', labelcolor=color)\n",
    "plt.title('MSE and R$^2$ vs Number of Trees')\n",
    "ax2 = plt.gca().twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('MSE', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(n_trees, MSE_test_rnd_forest, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.savefig('figs/MSE_R2_vs_trees.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize = [10,10])\n",
    "plt.plot(Bias_trees,label = 'Bias')\n",
    "plt.plot(variance_trees,label = 'Variance')\n",
    "\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title('Bias Variance')\n",
    "plt.legend(); plt.grid(); plt.semilogy()\n",
    "plt.savefig('figs/bias_variance_trees.png')\n",
    "plt.show()\n",
    "#delete to save memory\n",
    "del y_pred_basin_n_trees\n",
    "del y_pred_test_n_trees \n",
    "del y_pred_train_n_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Find MSE and R2 as function of depth\"\"\"\n",
    "n_depth = np.arange(1,50)\n",
    "#y_pred_basin_n_d = np.loadtxt(\"test_runs/y_pred_basin_n_d.txt\")\n",
    "#y_pred_test_n_d = np.loadtxt(\"test_runs/y_pred_test_n_d.txt\")\n",
    "#y_pred_train_n_d = np.loadtxt(\"test_runs/y_pred_train_n_d.txt\")\n",
    "#y_test = np.loadtxt('test_runs/y_test.txt')\n",
    "#y_train = np.loadtxt('test_runs/y_train.txt')\n",
    "\n",
    "Bias_d = np.zeros(len(n_depth))\n",
    "variance_d = np.zeros(len(n_depth))\n",
    "\n",
    "\"\"\"Find MSE as function of depth\"\"\"\n",
    "MSE_basin_rnd_forest_d = np.zeros(len(n_depth))\n",
    "MSE_test_rnd_forest_d = np.zeros(len(n_depth))\n",
    "MSE_train_rnd_forest_d = np.zeros(len(n_depth))\n",
    "R2_test_rnd_forest_d = np.zeros(len(n_depth))\n",
    "for i in range(len(n_depth)):\n",
    "    MSE_basin_rnd_forest_d[i] = metric.mean_squared_error(D_clean[:,0],y_pred_basin_n_d[:,i])\n",
    "    MSE_train_rnd_forest_d[i] = metric.mean_squared_error(y_train,y_pred_train_n_d[:,i])\n",
    "    MSE_test_rnd_forest_d[i] = metric.mean_squared_error(y_test,y_pred_test_n_d[:,i])\n",
    "    R2_test_rnd_forest_d[i] = metric.r2_score(y_test,y_pred_test_n_d[:,i])\n",
    "    Bias_d[i] = np.mean((y_test - np.mean(y_pred_test_n_d[:,i]))**2)\n",
    "    variance_d[i] = np.var(y_pred_test_n_d[:,i])\n",
    "    \n",
    "min_idx = np.argmin(MSE_test_rnd_forest_d)\n",
    "print(min_idx)\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize=(10,6))\n",
    "color = 'tab:red'\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('R$^2$', color=color)\n",
    "plt.plot(n_depth,R2_test_rnd_forest_d, color=color)\n",
    "plt.tick_params(axis='y', labelcolor=color)\n",
    "plt.title('MSE and R$^2$ vs Maximum depth')\n",
    "ax2 = plt.gca().twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('MSE', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(n_depth, MSE_test_rnd_forest_d, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.plot(n_depth[min_idx],MSE_test_rnd_forest_d[min_idx],'*',color = 'r')\n",
    "plt.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.savefig('figs/MSE_R2_vs_depth.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize = [10,10])\n",
    "plt.plot(Bias_d,label = 'Bias')\n",
    "plt.plot(variance_d,label = 'Variance')\n",
    "plt.plot(MSE_test_rnd_forest_d,label = 'Test MSE')\n",
    "plt.xlabel(\"Depth\",fontsize=\"large\")\n",
    "\n",
    "plt.title('Bias variance')\n",
    "plt.legend(); plt.grid(); plt.semilogy()\n",
    "plt.savefig('figs/Bias_variance_v_depth.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Run the algorithm for all ASO passes with optimized hyperparameters\"\"\"\n",
    "X = np.transpose(np.array([svf_clean,z_clean,northness_clean,eastness_clean,slp_clean,cc_clean]))#create matrix of all predicators\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,D_clean,test_size=0.8)\n",
    "\n",
    "depth = 16\n",
    "n_trees = 20\n",
    "MSE_basin = np.zeros(16)\n",
    "MSE_test = np.zeros(16)\n",
    "MSE_train = np.zeros(16)\n",
    "\n",
    "y_test_predictions = np.zeros(y_test.shape)\n",
    "y_train_predictions = np.zeros(y_train.shape)\n",
    "y_basin_predictions = np.zeros(D_clean.shape)\n",
    "R2_test = np.zeros(16)\n",
    "print(y_train.shape)\n",
    "for i in range(16):\n",
    "    print(i)\n",
    "    ypred_basin,ypred_test, ypred_train = skl_rfr(X,x_train,x_test,y_train[:,i],depth,n_trees)\n",
    "    MSE_basin[i] = metric.mean_squared_error(D_clean[:,i],ypred_basin)\n",
    "    MSE_train[i] = metric.mean_squared_error(y_train[:,i],ypred_train)\n",
    "    MSE_test[i] = metric.mean_squared_error(y_test[:,i],ypred_test)\n",
    "    R2_test[i] = metric.r2_score(y_test[:,i],ypred_test)\n",
    "    y_test_predictions[:,i] = ypred_test\n",
    "    y_train_predictions[:,i] =  ypred_train\n",
    "    y_basin_predictions[:,i] = ypred_basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = [] \n",
    "for i in range(16):\n",
    "    date.append(np.str(DOY[i]) + '/' + np.str(year[i]))\n",
    "    \n",
    "\"\"\"Find the total SWE for each year\"\"\"\n",
    "D_sum = np.zeros(16)\n",
    "for i in range(16):\n",
    "    h = D[:,:,i][np.logical_not(np.isnan(D[:,:,i]))]\n",
    "    D_sum[i] = np.sum(h)\n",
    "\n",
    "D_max_idx = np.argmax(D_sum)\n",
    "D_min_idx = np.argmin(D_sum)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "color = 'tab:red'\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Basin SWE [m]', color=color)\n",
    "plt.plot(D_sum, color=color)\n",
    "plt.tick_params(axis='y', labelcolor=color)\n",
    "plt.xticks( np.arange(16), date ,rotation = 45)\n",
    "plt.title('Total SWE and MSE')\n",
    "ax2 = plt.gca().twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('MSE', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot( MSE_test, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.savefig('figs/MSE_SWE_RF.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "color = 'tab:red'\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('$R^2$', color=color)\n",
    "plt.plot(R2_test, color=color)\n",
    "plt.tick_params(axis='y', labelcolor=color)\n",
    "plt.xticks( np.arange(16), date ,rotation = 45)\n",
    "plt.title('R2 score and MSE')\n",
    "ax2 = plt.gca().twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('MSE [m]', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot( MSE_test, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/R2_MSE_RF.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map back to coordinates for visualization\n",
    "import matplotlib as mpl\n",
    "def visual_map(i,save_or_not):\n",
    "    D_pred=np.zeros_like(D)\n",
    "    D_pred[not_nan,i]=y_basin_predictions[:,i]\n",
    "    D_pred[~not_nan,i]=np.nan\n",
    "    \n",
    "    cmap = plt.cm.jet  # define the colormap\n",
    "    # extract all colors from the .jet map\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    # force the first color entry to be grey\n",
    "    cmaplist[0] = (.5, .5, .5, 1.0)\n",
    "    # create the new map\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, cmap.N)\n",
    "    # define the bins and normalize\n",
    "    bounds = np.linspace(0, 0.8, 14)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    plt.figure()\n",
    "    cm=plt.imshow((np.abs(D_pred[:,:,i]-D[:,:,i])),cmap=cmap,norm = norm)\n",
    "    plt.title('Error, Year: '+str(year[i])+' DOY: ' + str(DOY[i]))\n",
    "    plt.ylabel('Row index, i')\n",
    "    plt.xlabel('Column index, j')\n",
    "    plt.clim(0, 0.7)\n",
    "    plt.colorbar()\n",
    "    if save_or_not == True:\n",
    "        (print('Image saved'))\n",
    "        plt.savefig('RF_Map_error' +str(year[i])+'_DOY_' + str(DOY[i])+'.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    cm=plt.imshow((D_pred[:,:,i]))\n",
    "    plt.title('Emulated Year: '+str(year[i])+' DOY: ' + str(DOY[i]))\n",
    "    plt.ylabel('Row index, i')\n",
    "    plt.xlabel('Column index, j')\n",
    "    plt.clim(0, 1.5)\n",
    "    plt.colorbar()\n",
    "    if save_or_not == True:\n",
    "        (print('Image saved'))\n",
    "        plt.savefig('RF_Map_emulated' +str(year[i])+'_DOY_' + str(DOY[i])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    cm=plt.imshow((D[:,:,i]))\n",
    "    plt.title('ASO Year: '+str(year[i])+' DOY: ' + str(DOY[i]))\n",
    "    plt.ylabel('Row index, i')\n",
    "    plt.xlabel('Column index, j')\n",
    "    plt.clim(0, 1.5)\n",
    "    plt.colorbar()\n",
    "    if save_or_not == True:\n",
    "        (print('Image saved'))\n",
    "        plt.savefig('RF_Map_true' +str(year[i])+'_DOY_' + str(DOY[i])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "for i in range(D.shape[2]):\n",
    "    visual_map(i,False)\n",
    "\n",
    "    #plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save selected runs for report\"\"\"\n",
    "#12 107/2019\n",
    "#3 117/2016\n",
    "#10 208/2017\n",
    "visual_map(12,True)\n",
    "visual_map(3,True)\n",
    "visual_map(10,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
